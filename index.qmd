---
title: Time Series Forecasting Comparison
subtitle: XGBoost vs AutoRegression on Multiple Datasets
authors:
  - name: Calvin Carter
    affiliation: UC Berkeley
    roles: writing
    corresponding: true
bibliography: references.bib
---

## Motivation

This comparison between XGBoost and AutoRegression in future predictions of power consumption in the Eastern United States and monthly average sunspot numbers is motivated by curiosity about the relative performance of these two models given non-linear relationships and interactions between features. 

Historically, XGBoost has been used in a variety of applications, with time series forecasting not being an exception. However, AutoRegression is a more traditional approach to time series forecasting, and might outperform XGBoost in a setting with only one source of information: the past values of the time series.

## Datasets

In this analysis, we compare XGBoost and AutoRegression models on two different time series datasets:

### 1. PJME Hourly Power Consumption

PMJ Interconnection LLC (PMJ) is a regional transmission organization (RTO) operating an electric transmission system serving all or part of many states in the Eastern United States. It is considered the largest independent power grid operator in North America [@pv2025pjm].

The dataset contains hourly power consumption data from PJM. The dataset spans 2002 to 2018 and contains 145,336 hourly records. [@wqg6-9917-25]

### 2. Monthly Sunspot Numbers (SN_m_tot_V2.0)

This dataset contains monthly mean total sunspot numbers from the World Data Center SILSO, Royal Observatory of Belgium, Brussels, spanning from 1749 to present. This provides a longer historical perspective on solar cycles, which typically last about 11 years. [@SILSO_monthly_sunspot]

## Analysis

In this analysis, we would like to see whether an XGBoost approach to Autoregression performs better or worse than a typical AR model (i.e. $y_t = \phi_0 + \phi_1 y_{t-1} + \dots + \phi_p y_{t-p} + \epsilon_t$). For each dataset and method, we will compare the two by allotting a test set of the final $\approx 10\%$ of the data.

For both methods, we will use the Mean Squared Error (MSE) as our metric for comparison. Additionally, we will be testing p values from 1 to 40.

### Method 1: Rolling predictions

The typical modeling pipeline in the Scikit-learn python package makes predictions based on a set number of features $X = (X_1^i,\dots,X_p^i)$ which are typically available when creating future predictions $\hat{y}_{n+1}, \dots \hat{y}_{n+m}$. However, in our autoregressive approach, our predictor variables are the features, i.e. $X = (X_1^i,\dots,X_p^i) = \hat{y}_{i-p}, \dots, \hat{y}_{i-1}$. Thus, our features will be entirely created by the models themselves after $p$ predictions. 

In this method, we will assume that the models will not have access to true $y$ values after $y_{n}$, where $n$ is the size of the train set.

First, we will create a new class `xgb_model` (available in xgb_model.py), which implements the rolling prediction-style XGBoost model. The key here is the `forecast` method, which implements the rolling predictions.

```python
import xgboost as xgb
import numpy as np

class xgb_model:
    def __init__(self, p):
        self.md = xgb.XGBRegressor(
            # Hyperparameters
        )
        self.p = p
    
    def train_model(self, train):
        X_train, y_train = self._create_lag_features(data=train, n_lags=self.p)
        self.md.fit(X_train, y_train)
    
    def forecast(self, train, n_forecast):
        """
        Rolling predictions: similar to how forecast works 
        in autoregressive model.
        """
        preds = np.zeros(n_forecast)
        last_values = train[-self.p:].values.copy()
        for i in range(n_forecast):
            X_pred = last_values.reshape(1, -1)
            
            y_pred = self.md.predict(X_pred)[0]
            preds[i] = y_pred
            
            last_values = np.append(last_values[1:], y_pred)
        
        return preds

    def _create_lag_features(self, data, n_lags):
        X = np.array([
            data[i - n_lags:i].values for i in range(n_lags, len(data))
        ])
        y = data[n_lags:]
        return X, y
```

We then train this rolling XGBoost model and the AR model on $p$ value candidates from 1 to 40 and compare which has the lowest MSE on the test set.

```{python}
from utils import find_best_model_rolling
import config

# Run analysis for all datasets
for dataset_config in config.DATASETS:
    dataset = dataset_config['name']
    target = dataset_config['target']
    parser = dataset_config['parser']
    
    print(f"Dataset: {dataset}")
    results_rolling = find_best_model_rolling(
        dataset=dataset,
        target=target,
        p_candidates=config.P_CANDIDATES,
        prop_test=config.TEST_PROP,
        parser=parser
    )
    print()
    print('----------------')
    print()
```

#### Results: PJME Hourly Power Consumption

For the PJME hourly power consumption dataset, we see the best model according to the selected metric is the rolling XGBoost with a Root mean squared error significantly lower than the best performing AR model.

![](figs/plot-rolling-PJME_hourly.png)

Here we can see that the rolling XGBoost model outperforms the AR model with bolder predictions on the test data. While neither model is able to capture the seasonal trend seen in the true data, the XGBoost model performs better overall.

#### Results: Monthly Average Sunspot Numbers

For the monthly average sunspot number dataset, we see the best model according to the selected metric is the rolling XGBoost with a Root mean squared error almost half that of the best performing AR model.

![](figs/plot-rolling-SN_m_tot_V2.0.png)

In this dataset (unlike in the PJME dataset), the rolling XGBoost model outperforms the AR model and is able to effectively capture the sinusoidal trend seen in the true data. This is likely due to there being less noise for XGBoost to overfit to.


### Method 2: Up-to-date Approach

In the typical autoregression pipeline, the model creates rolling predictions, and only has information available from the train set. In this approach, we will allow both the AR model and the XGBoost model to use the true $y$ values before $y_t$ instead of their own predictions.

To implement this, we will create a new method in the `xgb_model` class called `predict` which implements the up-to-date approach:

```python
class xgb_model:
...
    def predict(self, full_series, train_size):
        """
        Up-to-date approach: uses true y values before y_t 
        instead of rolling predictions.
        """
        X_test, _ = self._create_lag_features(full_series[train_size - self.p:], self.p)
        return self.md.predict(X_test)
...
```

Additionally, we will have to adjust AR model to use the true $y$ values before $y_t$ instead of their own predictions. We will create a new class `ar_model` (available in ar_model.py), which implements the up-to-date AR model:

```python
class ar_model:
...
    def predict(self, full_series, train_size):
        test_size = len(full_series) - train_size
        data_block = full_series[train_size - self.p:]
        X_test = np.array([
            data_block[i : i + self.p][::-1] 
            for i in range(test_size)
        ])
        params = self.md.params
        intercept = params[0]
        coeffs = params[1:]
        preds = intercept + np.dot(X_test, coeffs)
        return preds
...
```

We then train this up-to-date XGBoost model and the AR model on $p$ value candidates from 1 to 40 and compare which has the lowest MSE on the test set.

```{python}
from utils import find_best_model_uptodate

for dataset_config in config.DATASETS:
    dataset = dataset_config['name']
    target = dataset_config['target']
    parser = dataset_config['parser']

    print(f"Dataset: {dataset}")
    print()
    results_uptodate = find_best_model_uptodate(
        dataset=dataset, 
        target=target, 
        p_candidates=config.P_CANDIDATES, 
        prop_test=config.TEST_PROP,
        parser=parser
    )
    print()
    print('----------------')
    print()
```

#### Results: PJME Hourly Power Consumption

With the up-to-date approach, both models have access to the true $y$ values up to $y_{t-1}$ when predicting $y_t$. This is a more realistic scenario when predictions are made one step at a time with updated information.

![](figs/plot-uptodate-PJME_hourly.png)

The up-to-date XGBoost model outperforms the up-to-date AR model with slightly closer predictions to the true data. Visually, it's hard to tell due to the chaotic noise in the data, but the metrics show that the XGBoost model performs better overall.

#### Results: Monthly Average Sunspot Numbers

For the monthly sunspot number dataset with up-to-date predictions, the up-to-date XGBoost model outperforms the AR model with slightly more accurate predictions on the test data, but the difference is not massive.

![](figs/plot-uptodate-SN_m_tot_V2.0.png)

Visually, both of these models perform well. Both capture the sinusoidal trend seen in the true data. The up-to-date XGBoost model outperforms the AR model with slightly more accurate predictions on the test data, but the difference is not visually significant.


## Conclusion

In this project, we compared the performance of XGBoost and AR models across two different time series datasets: PJME hourly electricity usage, and daily sunspot numbers. In both datasets, the XGBoost model shows a clear advantage over the AR model. This is because XGBoost is more able to capture non-linear relationships and interactions between variables, while the AR models are limited.

Overall, I was surprised by the results of this project. I was expecting to see the AR model perform better than the XGBoost model, especially on rolling predictions. But the XGBoost model consistently outperformed the AR model in both datasets.
