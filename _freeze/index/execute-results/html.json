{
  "hash": "b9b4534bce4fb2c81fb0a4db82f4c9c7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Time Series Forecasting Comparison\nsubtitle: XGBoost vs AutoRegression on Multiple Datasets\nauthors:\n  - name: Calvin Carter\n    affiliation: UC Berkeley\n    roles: writing\n    corresponding: true\nbibliography: references.bib\n---\n\n## Motivation\n\nThis comparison between XGBoost and AutoRegression in future predictions of power consumption in the Eastern United States and monthly average sunspot numbers is motivated by curiosity about the relative performance of these two models given non-linear relationships and interactions between features. \n\nHistorically, XGBoost has been used in a variety of applications, with time series forecasting not being an exception. However, AutoRegression is a more traditional approach to time series forecasting, and might outperform XGBoost in a setting with only one source of information: the past values of the time series.\n\n## Datasets\n\nIn this analysis, we compare XGBoost and AutoRegression models on two different time series datasets:\n\n### 1. PJME Hourly Power Consumption\n\nPMJ Interconnection LLC (PMJ) is a regional transmission organization (RTO) operating an electric transmission system serving all or part of many states in the Eastern United States. It is considered the largest independent power grid operator in North America [@pv2025pjm].\n\nThe dataset contains hourly power consumption data from PJM. The dataset spans 2002 to 2018 and contains 145,336 hourly records. [@wqg6-9917-25]\n\n### 2. Monthly Sunspot Numbers (SN_m_tot_V2.0)\n\nThis dataset contains monthly mean total sunspot numbers from the World Data Center SILSO, Royal Observatory of Belgium, Brussels, spanning from 1749 to present. This provides a longer historical perspective on solar cycles, which typically last about 11 years. [@SILSO_monthly_sunspot]\n\n## Analysis\n\nIn this analysis, we would like to see whether an XGBoost approach to Autoregression performs better or worse than a typical AR model (i.e. $y_t = \\phi_0 + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p} + \\epsilon_t$). For each dataset and method, we will compare the two by allotting a test set of the final $\\approx 10\\%$ of the data.\n\nFor both methods, we will use the Mean Squared Error (MSE) as our metric for comparison. Additionally, we will be testing p values from 1 to 40.\n\n### Method 1: Rolling predictions\n\nThe typical modeling pipeline in the Scikit-learn python package makes predictions based on a set number of features $X = (X_1^i,\\dots,X_p^i)$ which are typically available when creating future predictions $\\hat{y}_{n+1}, \\dots \\hat{y}_{n+m}$. However, in our autoregressive approach, our predictor variables are the features, i.e. $X = (X_1^i,\\dots,X_p^i) = \\hat{y}_{i-p}, \\dots, \\hat{y}_{i}$. Thus, our features will be entirely created by the models themselves after $p$ predictions. \n\nIn this method, we will assume that the models will not have access to true $y$ values after $y_{n}$, where $n$ is the size of the train set.\n\nFirst, we will create a new class `xgb_model` (available in xgb_model.py), which implements the rolling prediction-style XGBoost model. The key here is the `forecast` method, which implements the rolling predictions.\n\n```python\nimport xgboost as xgb\nimport numpy as np\n\nclass xgb_model:\n    def __init__(self, p):\n        self.md = xgb.XGBRegressor(\n            # Hyperparameters\n        )\n        self.p = p\n    \n    def train_model(self, train):\n        X_train, y_train = self._create_lag_features(data=train, n_lags=self.p)\n        self.md.fit(X_train, y_train)\n    \n    def forecast(self, train, n_forecast):\n        \"\"\"\n        Rolling predictions: similar to how forecast works \n        in autoregressive model.\n        \"\"\"\n        preds = np.zeros(n_forecast)\n        last_values = train[-self.p:].values.copy()\n        for i in range(n_forecast):\n            X_pred = last_values.reshape(1, -1)\n            \n            y_pred = self.md.predict(X_pred)[0]\n            preds[i] = y_pred\n            \n            last_values = np.append(last_values[1:], y_pred)\n        \n        return preds\n\n    def _create_lag_features(self, data, n_lags):\n        X = np.array([\n            data[i - n_lags:i].values for i in range(n_lags, len(data))\n        ])\n        y = data[n_lags:]\n        return X, y\n```\n\nWe then train this rolling XGBoost model and the AR model on $p$ value candidates from 1 to 40 and compare which has the lowest MSE on the test set.\n\n::: {#256689b0 .cell execution_count=1}\n``` {.python .cell-code .hidden}\nfrom utils import find_best_model_rolling\nimport config\n\n# Run analysis for all datasets\nfor dataset_config in config.DATASETS:\n    dataset = dataset_config['name']\n    target = dataset_config['target']\n    parser = dataset_config['parser']\n    \n    print(f\"Dataset: {dataset}\")\n    results_rolling = find_best_model_rolling(\n        dataset=dataset,\n        target=target,\n        p_candidates=config.P_CANDIDATES,\n        prop_test=config.TEST_PROP,\n        parser=parser\n    )\n    print()\n    print('----------------')\n    print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDataset: PJME_hourly\nBest XGB Model - Rolling (p=25)\nMSE: 28434789.9809\nRMSE: 5332.4282\nMAE: 4266.9572\nR_squared: 0.2332\n\nBest AR Model - Rolling (p=40)\nMSE: 37869846.0096\nRMSE: 6153.8481\nMAE: 4927.4554\nR_squared: -0.0212\n\nOverall Best Model: XGB with p=25 (MSE: 28434789.9809)\n\n----------------\n\nDataset: SN_m_tot_V2.0\nBest XGB Model - Rolling (p=31)\nMSE: 920.5393\nRMSE: 30.3404\nMAE: 23.6469\nR_squared: 0.7310\n\nBest AR Model - Rolling (p=39)\nMSE: 2639.6616\nRMSE: 51.3776\nMAE: 43.7312\nR_squared: 0.2285\n\nOverall Best Model: XGB with p=31 (MSE: 920.5393)\n\n----------------\n\n```\n:::\n:::\n\n\n:::{#a7f70871 .cell .markdown}\n#### Results: PJME Hourly Power Consumption\n\nFor the PJME hourly power consumption dataset, we see the best model according to the selected metric is the rolling XGBoost with a Root mean squared error significantly lower than the best performing AR model.\n\n![](figs/plot-rolling-PJME_hourly.png)\n\nHere we can see that the rolling XGBoost model outperforms the AR model with bolder predictions on the test data. While neither model is able to capture the seasonal trend seen in the true data, the XGBoost model performs better overall.\n\n#### Results: Monthly Average Sunspot Numbers\n\nFor the monthly average sunspot number dataset, we see the best model according to the selected metric is the rolling XGBoost with a Root mean squared error almost half that of the best performing AR model.\n\n![](figs/plot-rolling-SN_m_tot_V2.0.png)\n\nIn this dataset (unlike in the PJME dataset), the rolling XGBoost model outperforms the AR model and is able to effectively capture the sinusoidal trend seen in the true data. This is likely due to there being less noise for XGBoost to overfit to.\n\n\n### Method 2: Up-to-date Approach\n\nIn the typical autoregression pipeline, the model creates rolling predictions, and only has information available from the train set. In this approach, we will allow both the AR model and the XGBoost model to use the true $y$ values before $y_t$ instead of their own predictions.\n\nTo implement this, we will create a new method in the `xgb_model` class called `predict` which implements the up-to-date approach:\n\n```python\nclass xgb_model:\n...\n    def predict(self, full_series, train_size):\n        \"\"\"\n        Up-to-date approach: uses true y values before y_t \n        instead of rolling predictions.\n        \"\"\"\n        X_test, _ = self._create_lag_features(full_series[train_size - self.p:], self.p)\n        return self.md.predict(X_test)\n...\n```\n\nAdditionally, we will have to adjust AR model to use the true $y$ values before $y_t$ instead of their own predictions. We will create a new class `ar_model` (available in ar_model.py), which implements the up-to-date AR model:\n\n```python\nclass ar_model:\n...\n    def predict(self, full_series, train_size):\n        test_size = len(full_series) - train_size\n        data_block = full_series[train_size - self.p:]\n        X_test = np.array([\n            data_block[i : i + self.p][::-1] \n            for i in range(test_size)\n        ])\n        params = self.md.params\n        intercept = params[0]\n        coeffs = params[1:]\n        preds = intercept + np.dot(X_test, coeffs)\n        return preds\n...\n```\n\nWe then train this up-to-date XGBoost model and the AR model on $p$ value candidates from 1 to 40 and compare which has the lowest MSE on the test set.\n:::\n\n::: {#7a6dfd5f .cell execution_count=2}\n``` {.python .cell-code .hidden}\nfrom utils import find_best_model_uptodate\n\nfor dataset_config in config.DATASETS:\n    dataset = dataset_config['name']\n    target = dataset_config['target']\n    parser = dataset_config['parser']\n\n    print(f\"Dataset: {dataset}\")\n    print()\n    results_uptodate = find_best_model_uptodate(\n        dataset=dataset, \n        target=target, \n        p_candidates=config.P_CANDIDATES, \n        prop_test=config.TEST_PROP,\n        parser=parser\n    )\n    print()\n    print('----------------')\n    print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nDataset: PJME_hourly\n\nBest XGB Model - Up-to-date (p=40)\nMSE: 512248.6589\nRMSE: 715.7155\nMAE: 436.6352\nR_squared: 0.9862\n\nBest AR Model - Up-to-date (p=40)\nMSE: 643015.8163\nRMSE: 801.8827\nMAE: 424.0543\nR_squared: 0.9827\n\nOverall Best Model: XGB with p=40 (MSE: 512248.6589)\n\n----------------\n\nDataset: SN_m_tot_V2.0\n\nBest XGB Model - Up-to-date (p=35)\nMSE: 389.8788\nRMSE: 19.7453\nMAE: 14.0446\nR_squared: 0.8861\n\nBest AR Model - Up-to-date (p=31)\nMSE: 403.9939\nRMSE: 20.0996\nMAE: 14.3172\nR_squared: 0.8819\n\nOverall Best Model: XGB with p=35 (MSE: 389.8788)\n\n----------------\n\n```\n:::\n:::\n\n\n:::{#580a6ab8 .cell .markdown}\n#### Results: PJME Hourly Power Consumption\n\nWith the up-to-date approach, both models have access to the true $y$ values up to $y_{t-1}$ when predicting $y_t$. This is a more realistic scenario when predictions are made one step at a time with updated information.\n\n![](figs/plot-uptodate-PJME_hourly.png)\n\nThe up-to-date XGBoost model outperforms the up-to-date AR model with slightly closer predictions to the true data. Visually, it's hard to tell due to the chaotic noise in the data, but the metrics show that the XGBoost model performs better overall.\n\n#### Results: Monthly Average Sunspot Numbers\n\nFor the monthly sunspot number dataset with up-to-date predictions, the up-to-date XGBoost model outperforms the AR model with slightly more accurate predictions on the test data, but the difference is not massive.\n\n![](figs/plot-uptodate-SN_m_tot_V2.0.png)\n\nVisually, both of these models perform well. Both capture the sinusoidal trend seen in the true data. The up-to-date XGBoost model outperforms the AR model with slightly more accurate predictions on the test data, but the difference is not visually significant.\n\n\n## Conclusion\n\nIn this project, we compared the performance of XGBoost and AR models across two different time series datasets: PJME hourly electricity usage, and daily sunspot numbers. In both datasets, the XGBoost model shows a clear advantage over the AR model. This is because XGBoost is more able to capture non-linear relationships and interactions between variables, while the AR models are limited.\n\nOverall, I was surprised by the results of this project. I was expecting to see the AR model perform better than the XGBoost model, especially on rolling predictions. But the XGBoost model consistently outperformed the AR model in both datasets.\n:::\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}