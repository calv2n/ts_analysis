{
  "hash": "4e3546567d3ce81c5fe3da396d45e7d6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Power Consumption in the Eastern United States\nsubtitle: A Time Series Analysis\nauthors:\n  - name: Calvin Carter\n    affiliation: UC Berkeley\n    roles: writing\n    corresponding: true\nbibliography: references.bib\n---\n\n## Motivation\n\nA comparison between XGBoost and AutoRegression in future predictions\n\n## Dataset\n\nPMJ Interconnection LLC (PMJ) is a regional transmission organization (RTO) operating an electric transmission system serving all or part of many states in the Eastern United States. It is considered the the largest independent power grid operator in North America [@pv2025pjm].\n\nThe dataset we will use in our analysis contains the hourly and daily power consumption data from PJM. The dataset spans 2002 to 2018 and contains 145,336 hourly and 6,057 daily records. [@wqg6-9917-25]\n\n## Analysis\n\nIn this analyis, we would like to see whether an XGBoost approach to Autoregression performs better or worse than a typical AR model (i.e. $y_t = \\phi_0 + \\phi_1 y_{t-1} + \\dots + \\phi_p y_{t-p} + \\epsilon_t$). For each method, we will compare the two by allotting a test set of the final $\\approx 10\\%$ of daily power consumption data.\n\n### Method 1: Rolling predictions\n\nThe typical modeling pipeline in the Scikit-learn python package makes predictions based on a set number of features $X = (X_1^i,\\dots,X_p^i)$ which are typically available when creating future predictions $\\hat{y}_{n+1}, \\dots \\hat{y}_{n+m}$. However, in our autoregressive approach, our predictor variables are the the features, i.e. $X = (X_1^i,\\dots,X_p^i) = \\hat{y}_{i-p}, \\dots, \\hat{y}_{i}$. Thus, our features will be entirely created by the models themselves after $p$ predictions. \n\nIn this method, we will assume that the models will not have access to true $y$ values after $y_{n}$, where $n$ is the size of the train set.\n\nFirst, we will create a new class `xgb_model` (available in xgb_model.py), which impliments the rolling prediction-style XGBoost model. The key here is the `forecast` method, which implements the rolling predictions.\n\n```python\nimport xgboost as xgb\nimport numpy as np\n\nclass xgb_model:\n    def __init__(self, p):\n        self.md = xgb.XGBRegressor(\n            # Hyperparameters\n        )\n        self.p = p\n    \n    def train_model(self, train):\n        X_train, y_train = self._create_lag_features(data=train, n_lags=self.p)\n        self.md.fit(X_train, y_train)\n    \n    def forecast(self, train, n_forecast):\n        \"\"\"\n        Rolling predictions: similar to how forecast works \n        in autoregressive model.\n        \"\"\"\n        preds = np.zeros(n_forecast)\n        last_values = train[-self.p:].values.copy()\n        for i in range(n_forecast):\n            X_pred = last_values.reshape(1, -1)\n            \n            y_pred = self.md.predict(X_pred)[0]\n            preds[i] = y_pred\n            \n            last_values = np.append(last_values[1:], y_pred)\n        \n        return preds\n\n    def _create_lag_features(self, data, n_lags):\n        X = np.array([\n            data[i - n_lags:i].values for i in range(n_lags, len(data))\n        ])\n        y = data[n_lags:]\n        return X, y\n```\n\nWe then train this rolling XGBoost model and the AR model on $p$ value candidates from 1 to 40 and compare which has the lowest MSE on the test set. The results:\n\n::: {#de20e68c .cell execution_count=1}\n``` {.python .cell-code .hidden}\nfrom utils import find_best_model_rolling, plot_best_rolling\nimport config\n\nresults_rolling = find_best_model_rolling(\n    dataset=config.DATASET, \n    target=config.TARGET, \n    p_candidates=config.P_CANDIDATES, \n    prop_test=config.TEST_PROP\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest XGB Model - Rolling (p=25)\nMSE: 28434789.9809\nRMSE: 5332.4282\nMAE: 4266.9572\nR_squared: 0.2332\n\nBest AR Model - Rolling (p=40)\nMSE: 37869846.0096\nRMSE: 6153.8481\nMAE: 4927.4554\nR_squared: -0.0212\n\nOverall Best Model: XGB with p=25 (MSE: 28434789.9809)\n```\n:::\n:::\n\n\n:::{#7a29cb29 .cell .markdown}\nWe see the best model according to the selected metric is the rolling XGBoost on $y_n,\\dots,y_{n-25}$ with a Root mean squared error almost 1000 less than the best performing AR model.\n\nLet's plot each model's predictions against the real data:\n:::\n\n::: {#f978234e .cell execution_count=2}\n``` {.python .cell-code .hidden}\nplot_best_rolling(results_rolling)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=610 height=449}\n:::\n:::\n\n\n:::{#8af93668 .cell .markdown}\nAlthough both models fail to capture the seasonal sinusoidal aspect of the test data, the XGBoost predictions are bolder and effectively capture the pattern in baseline variance. While the AR(40) model provides decent predictions, it uses a constant prediction after around 2000 hours.\n\n### Method 2: Up-to-date Approach\n\nIn the typical autoregression pipeline, the model creates rolling predictions, and only has information available from the train set. In this approach, we will allow both the AR model and the XGBoost model to use the true $y$ values before $y_t$ instead of their own predictions.\n\nTo implement this, we will create a new method in the `xgb_model` class called `predict` which implements the up-to-date approach:\n\n```python\nclass xgb_model:\n...\n    def predict(self, full_series, train_size):\n        \"\"\"\n        Up-to-date approach: uses true y values before y_t \n        instead of rolling predictions.\n        \"\"\"\n        X_test, _ = self._create_lag_features(full_series[train_size - self.p:], self.p)\n        return self.md.predict(X_test)\n...\n```\n\nAdditionally, we will have to adjust AR model to use the true $y$ values before $y_t$ instead of their own predictions. We will create a new class `ar_model` (available in ar_model.py), which impliments the up-to-date AR model.\n\nWe then train this up-to-date XGBoost model and the AR model on $p$ value candidates from 1 to 40 and compare which has the lowest MSE on the test set. The results:\n:::\n\n::: {#521f3545 .cell execution_count=3}\n``` {.python .cell-code .hidden}\nfrom utils import find_best_model_uptodate, plot_best_uptodate\n\nresults_uptodate = find_best_model_uptodate(\n    dataset=config.DATASET, \n    target=config.TARGET, \n    p_candidates=config.P_CANDIDATES, \n    prop_test=config.TEST_PROP\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBest XGB Model - Up-to-date (p=40)\nMSE: 512248.6589\nRMSE: 715.7155\nMAE: 436.6352\nR_squared: 0.9862\n\nBest AR Model - Up-to-date (p=40)\nMSE: 643015.8163\nRMSE: 801.8827\nMAE: 424.0543\nR_squared: 0.9827\n\nOverall Best Model: XGB with p=40 (MSE: 512248.6589)\n```\n:::\n:::\n\n\n:::{#e2f99470 .cell .markdown}\nWith the up-to-date approach, both models have access to the true $y$ values up to $y_{t-1}$ when predicting $y_t$. This is a more realistic scenario when predictions are made one step at a time with updated information. We can see here that XGBoost really shines. While the AR model performs well \n\nLet's visualize the results:\n:::\n\n::: {#dd775201 .cell execution_count=4}\n``` {.python .cell-code .hidden}\nplot_best_uptodate(results_uptodate)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=610 height=449}\n:::\n:::\n\n\n:::{#b28c77f3 .cell .markdown}\nInterestingly: we see a more comparable story between the two models in this setting. However, the XGBoost model still outperforms the AR model. Although\n:::\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}